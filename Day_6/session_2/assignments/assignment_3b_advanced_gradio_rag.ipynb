{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Assignment 3b: Advanced Gradio RAG Frontend\n",
        "## Day 6 Session 2 - Building Configurable RAG Applications\n",
        "\n",
        "In this assignment, you'll extend your basic RAG interface with advanced configuration options to create a professional, feature-rich RAG application.\n",
        "\n",
        "**New Features to Add:**\n",
        "- Model selection dropdown (gpt-4o, gpt-4o-mini)\n",
        "- Temperature slider (0 to 1 with 0.1 intervals)\n",
        "- Chunk size configuration\n",
        "- Chunk overlap configuration  \n",
        "- Similarity top-k slider\n",
        "- Node postprocessor multiselect\n",
        "- Similarity cutoff slider\n",
        "- Response synthesizer multiselect\n",
        "\n",
        "**Learning Objectives:**\n",
        "- Advanced Gradio components and interactions\n",
        "- Dynamic RAG configuration\n",
        "- Professional UI design patterns\n",
        "- Parameter validation and handling\n",
        "- Building production-ready AI applications\n",
        "\n",
        "**Prerequisites:**\n",
        "- Completed Assignment 3a (Basic Gradio RAG)\n",
        "- Understanding of RAG parameters and their effects\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìö Part 1: Setup and Imports\n",
        "\n",
        "Import all necessary libraries including advanced RAG components for configuration options.\n",
        "\n",
        "**Note:** This assignment uses OpenRouter for LLM access (not OpenAI). Make sure you have your `OPENROUTER_API_KEY` environment variable set.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/ishandutta/miniconda3/envs/accelerator/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ All libraries imported successfully!\n"
          ]
        }
      ],
      "source": [
        "# Import all required libraries\n",
        "import gradio as gr\n",
        "import os\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Optional, Any\n",
        "\n",
        "# LlamaIndex core components\n",
        "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, StorageContext, Settings\n",
        "from llama_index.vector_stores.lancedb import LanceDBVectorStore\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.llms.openrouter import OpenRouter\n",
        "\n",
        "# Advanced RAG components\n",
        "from llama_index.core.postprocessor import SimilarityPostprocessor\n",
        "from llama_index.core.response_synthesizers import TreeSummarize, Refine, CompactAndRefine\n",
        "from llama_index.core.retrievers import VectorIndexRetriever\n",
        "\n",
        "print(\"‚úÖ All libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ü§ñ Part 2: Advanced RAG Backend Class\n",
        "\n",
        "Create an advanced RAG backend that supports dynamic configuration of all parameters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Advanced RAG Backend initialized and ready!\n"
          ]
        }
      ],
      "source": [
        "class AdvancedRAGBackend:\n",
        "    \"\"\"Advanced RAG backend with configurable parameters.\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.index = None\n",
        "        self.available_models = [\"gpt-4o\", \"gpt-4o-mini\"]\n",
        "        self.available_postprocessors = [\"SimilarityPostprocessor\"]\n",
        "        self.available_synthesizers = [\"TreeSummarize\", \"Refine\", \"CompactAndRefine\", \"Default\"]\n",
        "        self.update_settings()\n",
        "        \n",
        "    def update_settings(self, model: str = \"gpt-4o-mini\", temperature: float = 0.1, chunk_size: int = 512, chunk_overlap: int = 50):\n",
        "        \"\"\"Update LlamaIndex settings based on user configuration.\"\"\"\n",
        "        # Set up the LLM using OpenRouter\n",
        "        api_key = os.getenv(\"OPENROUTER_API_KEY\")\n",
        "        if api_key:\n",
        "            Settings.llm = OpenRouter(\n",
        "                api_key=api_key,\n",
        "                model=model,\n",
        "                temperature=temperature\n",
        "            )\n",
        "        \n",
        "        # Set up the embedding model (keep this constant)\n",
        "        Settings.embed_model = HuggingFaceEmbedding(\n",
        "            model_name=\"BAAI/bge-small-en-v1.5\",\n",
        "            trust_remote_code=True\n",
        "        )\n",
        "        \n",
        "        # Set chunking parameters from function parameters\n",
        "        Settings.chunk_size = chunk_size\n",
        "        Settings.chunk_overlap = chunk_overlap\n",
        "    \n",
        "    def initialize_database(self, data_folder=\"../data\"):\n",
        "        \"\"\"Initialize the vector database with documents.\"\"\"\n",
        "        # Check if data folder exists\n",
        "        if not Path(data_folder).exists():\n",
        "            return f\"‚ùå Data folder '{data_folder}' not found!\"\n",
        "        \n",
        "        try:\n",
        "            # Create vector store\n",
        "            vector_store = LanceDBVectorStore(\n",
        "                uri=\"./advanced_rag_vectordb\",\n",
        "                table_name=\"documents\"\n",
        "            )\n",
        "            \n",
        "            # Load documents\n",
        "            reader = SimpleDirectoryReader(input_dir=data_folder, recursive=True)\n",
        "            documents = reader.load_data()\n",
        "            \n",
        "            # Create storage context and index\n",
        "            storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "            self.index = VectorStoreIndex.from_documents(\n",
        "                documents, \n",
        "                storage_context=storage_context,\n",
        "                show_progress=True\n",
        "            )\n",
        "            \n",
        "            return f\"‚úÖ Database initialized successfully with {len(documents)} documents!\"\n",
        "        \n",
        "        except Exception as e:\n",
        "            return f\"‚ùå Error initializing database: {str(e)}\"\n",
        "    \n",
        "    def get_postprocessor(self, postprocessor_name: str, similarity_cutoff: float):\n",
        "        \"\"\"Get the selected postprocessor.\"\"\"\n",
        "        if postprocessor_name == \"SimilarityPostprocessor\":\n",
        "            return SimilarityPostprocessor(similarity_cutoff=similarity_cutoff)\n",
        "        elif postprocessor_name == \"None\":\n",
        "            return None\n",
        "        else:\n",
        "            return None\n",
        "    \n",
        "    def get_synthesizer(self, synthesizer_name: str):\n",
        "        \"\"\"Get the selected response synthesizer.\"\"\"\n",
        "        if synthesizer_name == \"TreeSummarize\":\n",
        "            return TreeSummarize()\n",
        "        elif synthesizer_name == \"Refine\":\n",
        "            return Refine()\n",
        "        elif synthesizer_name == \"CompactAndRefine\":\n",
        "            return CompactAndRefine()\n",
        "        elif synthesizer_name == \"Default\":\n",
        "            return None\n",
        "        else:\n",
        "            return None\n",
        "    \n",
        "    def advanced_query(self, question: str, model: str, temperature: float, \n",
        "                      chunk_size: int, chunk_overlap: int, similarity_top_k: int,\n",
        "                      postprocessor_names: List[str], similarity_cutoff: float,\n",
        "                      synthesizer_name: str) -> Dict[str, Any]:\n",
        "        \"\"\"Query the RAG system with advanced configuration.\"\"\"\n",
        "        \n",
        "        # Check if index exists\n",
        "        if self.index is None:\n",
        "            return {\"response\": \"‚ùå Please initialize the database first!\", \"sources\": [], \"config\": {}}\n",
        "        \n",
        "        # Check if question is empty\n",
        "        if not question or not question.strip():\n",
        "            return {\"response\": \"‚ö†Ô∏è Please enter a question first!\", \"sources\": [], \"config\": {}}\n",
        "        \n",
        "        try:\n",
        "            # Update settings with new parameters\n",
        "            self.update_settings(model, temperature, chunk_size, chunk_overlap)\n",
        "            \n",
        "            # Get postprocessors\n",
        "            postprocessors = []\n",
        "            for name in postprocessor_names:\n",
        "                processor = self.get_postprocessor(name, similarity_cutoff)\n",
        "                if processor is not None:\n",
        "                    postprocessors.append(processor)\n",
        "            \n",
        "            # Get synthesizer\n",
        "            synthesizer = self.get_synthesizer(synthesizer_name)\n",
        "            \n",
        "            # Create query engine with all parameters\n",
        "            query_engine_kwargs = {\"similarity_top_k\": similarity_top_k}\n",
        "            if postprocessors:\n",
        "                query_engine_kwargs[\"node_postprocessors\"] = postprocessors\n",
        "            if synthesizer is not None:\n",
        "                query_engine_kwargs[\"response_synthesizer\"] = synthesizer\n",
        "            \n",
        "            query_engine = self.index.as_query_engine(**query_engine_kwargs)\n",
        "            \n",
        "            # Query and get response\n",
        "            response = query_engine.query(question)\n",
        "            \n",
        "            # Extract source information if available\n",
        "            sources = []\n",
        "            if hasattr(response, 'source_nodes'):\n",
        "                for node in response.source_nodes:\n",
        "                    sources.append({\n",
        "                        \"text\": node.text[:200] + \"...\",\n",
        "                        \"score\": getattr(node, 'score', 0.0),\n",
        "                        \"source\": getattr(node.node, 'metadata', {}).get('file_name', 'Unknown')\n",
        "                    })\n",
        "            \n",
        "            return {\n",
        "                \"response\": str(response),\n",
        "                \"sources\": sources,\n",
        "                \"config\": {\n",
        "                    \"model\": model,\n",
        "                    \"temperature\": temperature,\n",
        "                    \"chunk_size\": chunk_size,\n",
        "                    \"chunk_overlap\": chunk_overlap,\n",
        "                    \"similarity_top_k\": similarity_top_k,\n",
        "                    \"postprocessors\": postprocessor_names,\n",
        "                    \"similarity_cutoff\": similarity_cutoff,\n",
        "                    \"synthesizer\": synthesizer_name\n",
        "                }\n",
        "            }\n",
        "        \n",
        "        except Exception as e:\n",
        "            return {\"response\": f\"‚ùå Error processing query: {str(e)}\", \"sources\": [], \"config\": {}}\n",
        "\n",
        "# Initialize the backend\n",
        "rag_backend = AdvancedRAGBackend()\n",
        "print(\"üöÄ Advanced RAG Backend initialized and ready!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üé® Part 3: Advanced Gradio Interface\n",
        "\n",
        "Create a sophisticated Gradio interface with all the configuration options specified:\n",
        "1. Database initialization button\n",
        "2. Search query input and button  \n",
        "3. Model selection dropdown\n",
        "4. Temperature slider\n",
        "5. Chunk size and overlap inputs\n",
        "6. Similarity top-k slider\n",
        "7. Node postprocessor multiselect\n",
        "8. Similarity cutoff slider\n",
        "9. Response synthesizer multiselect\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_advanced_rag_interface():\n",
        "    \"\"\"Create advanced RAG interface with full configuration options.\"\"\"\n",
        "    \n",
        "    def initialize_db():\n",
        "        \"\"\"Handle database initialization.\"\"\"\n",
        "        return rag_backend.initialize_database()\n",
        "    \n",
        "    def handle_advanced_query(question, model, temperature, chunk_size, chunk_overlap, \n",
        "                             similarity_top_k, postprocessors, similarity_cutoff, synthesizer):\n",
        "        \"\"\"Handle advanced RAG queries with all configuration options.\"\"\"\n",
        "        result = rag_backend.advanced_query(\n",
        "            question, model, temperature, chunk_size, chunk_overlap,\n",
        "            similarity_top_k, postprocessors, similarity_cutoff, synthesizer\n",
        "        )\n",
        "        \n",
        "        # Format configuration for display\n",
        "        config_text = f\"\"\"**Current Configuration:**\n",
        "- Model: {result['config'].get('model', 'N/A')}\n",
        "- Temperature: {result['config'].get('temperature', 'N/A')}\n",
        "- Chunk Size: {result['config'].get('chunk_size', 'N/A')}\n",
        "- Chunk Overlap: {result['config'].get('chunk_overlap', 'N/A')}\n",
        "- Similarity Top-K: {result['config'].get('similarity_top_k', 'N/A')}\n",
        "- Postprocessors: {', '.join(result['config'].get('postprocessors', []))}\n",
        "- Similarity Cutoff: {result['config'].get('similarity_cutoff', 'N/A')}\n",
        "- Synthesizer: {result['config'].get('synthesizer', 'N/A')}\"\"\"\n",
        "        \n",
        "        return result[\"response\"], config_text\n",
        "    \n",
        "    # TODO: Create the advanced interface structure\n",
        "    # Hint: This interface needs more complex layout with configuration controls\n",
        "    \n",
        "    with gr.Blocks(title=\"Advanced RAG Assistant\") as interface:\n",
        "        # TODO: Add title and description\n",
        "        # Hint: Use gr.Markdown() for formatted text\n",
        "        \n",
        "        # Your title and description here:\n",
        "        \n",
        "        \n",
        "        # TODO: Add database initialization section\n",
        "        # Hint: Use gr.Button() for initialization and gr.Textbox() for status\n",
        "        # init_btn = ?\n",
        "        # status_output = ?\n",
        "        \n",
        "        \n",
        "        # TODO: Create main layout with columns\n",
        "        # Hint: Configuration controls on left, query/response on right makes sense\n",
        "        # Use gr.Row() and gr.Column() to organize this\n",
        "        \n",
        "        with gr.Row():\n",
        "            with gr.Column(scale=1):\n",
        "                \n",
        "                gr.Markdown(\"### ‚öôÔ∏è RAG Configuration\")\n",
        "                \n",
        "                # TODO: Model selection\n",
        "                # Hint: Use gr.Dropdown() with choices=[\"gpt-4o\", \"gpt-4o-mini\"]\n",
        "                # model_dropdown = ?\n",
        "                \n",
        "                \n",
        "                # TODO: Temperature control  \n",
        "                # Hint: Use gr.Slider() with minimum=0.0, maximum=1.0, step=0.1, value=0.1\n",
        "                # temperature_slider = ?\n",
        "                \n",
        "                \n",
        "                # TODO: Chunking parameters\n",
        "                # Hint: Use gr.Number() for numeric inputs with default values\n",
        "                # chunk_size_input = ?  (default 512)\n",
        "                \n",
        "                # chunk_overlap_input = ?  (default 50)\n",
        "                \n",
        "                \n",
        "                # TODO: Retrieval parameters\n",
        "                # Hint: Use gr.Slider() with minimum=1, maximum=20, step=1, value=5\n",
        "                # similarity_topk_slider = ?\n",
        "                \n",
        "                \n",
        "                # TODO: Postprocessor selection\n",
        "                # Hint: Use gr.CheckboxGroup() with choices=[\"SimilarityPostprocessor\"]\n",
        "                # postprocessor_checkbox = ?\n",
        "                \n",
        "                \n",
        "                # TODO: Similarity filtering\n",
        "                # Hint: Use gr.Slider() with minimum=0.0, maximum=1.0, step=0.1, value=0.3\n",
        "                # similarity_cutoff_slider = ?\n",
        "                \n",
        "                \n",
        "                # TODO: Response synthesizer\n",
        "                # Hint: Use gr.Dropdown() with choices=[\"TreeSummarize\", \"Refine\", \"CompactAndRefine\", \"Default\"]\n",
        "                # synthesizer_dropdown = ?\n",
        "                \n",
        "            \n",
        "            with gr.Column(scale=2):\n",
        "                gr.Markdown(\"### üí¨ Query Interface\")\n",
        "                \n",
        "                # TODO: Query input\n",
        "                # Hint: Use gr.Textbox() with label=\"Ask a question\", placeholder text, lines=3\n",
        "                # query_input = ?\n",
        "                \n",
        "                \n",
        "                # TODO: Submit button\n",
        "                # Hint: Use gr.Button() with variant=\"primary\"\n",
        "                # submit_btn = ?\n",
        "                \n",
        "                \n",
        "                # TODO: Response output\n",
        "                # Hint: Use gr.Textbox() with lines=12, interactive=False\n",
        "                # response_output = ?\n",
        "                \n",
        "                \n",
        "                # TODO: Configuration display\n",
        "                # Hint: Use gr.Textbox() with lines=8, interactive=False\n",
        "                # config_display = ?\n",
        "        \n",
        "        \n",
        "        # Uncomment to Connect functions to components\n",
        "        # init_btn.click(initialize_db, outputs=[status_output])\n",
        "        \n",
        "        # submit_btn.click(\n",
        "        #     handle_advanced_query,\n",
        "        #     inputs=[\n",
        "        #         query_input, model_dropdown, temperature_slider,\n",
        "        #         chunk_size_input, chunk_overlap_input, similarity_topk_slider,\n",
        "        #         postprocessor_checkbox, similarity_cutoff_slider, synthesizer_dropdown\n",
        "        #     ],\n",
        "        #     outputs=[response_output, config_display]\n",
        "        # )\n",
        "        \n",
        "    \n",
        "    return interface\n",
        "\n",
        "# Create the interface\n",
        "advanced_interface = create_advanced_rag_interface()\n",
        "print(\"‚úÖ Advanced RAG interface created successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üöÄ Part 4: Launch Your Advanced Application\n",
        "\n",
        "Launch your advanced Gradio application and test all the configuration options!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"üéâ Launching your Advanced RAG Assistant...\")\n",
        "print(\"üîó Your application will open in a new browser tab!\")\n",
        "print(\"\")\n",
        "print(\"‚ö†Ô∏è  Make sure your OPENROUTER_API_KEY environment variable is set!\")\n",
        "print(\"\")\n",
        "print(\"üìã Testing Instructions:\")\n",
        "print(\"1. Click 'Initialize Vector Database' button first\")\n",
        "print(\"2. Wait for success message\")\n",
        "print(\"3. Configure your RAG parameters:\")\n",
        "print(\"   - Choose model (gpt-4o, gpt-4o-mini)\")\n",
        "print(\"   - Adjust temperature (0.0 = deterministic, 1.0 = creative)\")\n",
        "print(\"   - Set chunk size and overlap\")\n",
        "print(\"   - Choose similarity top-k\")\n",
        "print(\"   - Select postprocessors and synthesizer\")\n",
        "print(\"4. Enter a question and click 'Ask Question'\")\n",
        "print(\"5. Review both the response and configuration used\")\n",
        "print(\"\")\n",
        "print(\"üß™ Experiments to try:\")\n",
        "print(\"- Compare different models with the same question\")\n",
        "print(\"- Test temperature effects (0.1 vs 0.9)\")\n",
        "print(\"- Try different chunk sizes (256 vs 1024)\")\n",
        "print(\"- Compare synthesizers (TreeSummarize vs Refine)\")\n",
        "print(\"- Adjust similarity cutoff to filter results\")\n",
        "\n",
        "# Your code here:\n",
        "advanced_interface.launch()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üí° Understanding the Configuration Options\n",
        "\n",
        "### Model Selection\n",
        "- **gpt-4o**: Latest and most capable model, best quality responses\n",
        "- **gpt-4o-mini**: Faster and cheaper while maintaining good quality\n",
        "\n",
        "### Temperature (0.0 - 1.0)\n",
        "- **0.0-0.3**: Deterministic, factual responses\n",
        "- **0.4-0.7**: Balanced creativity and accuracy\n",
        "- **0.8-1.0**: More creative and varied responses\n",
        "\n",
        "### Chunk Size & Overlap\n",
        "- **Chunk Size**: How much text to process at once (256-1024 typical)\n",
        "- **Chunk Overlap**: Overlap between chunks to maintain context (10-100 typical)\n",
        "\n",
        "### Similarity Top-K (1-20)\n",
        "- **Lower values (3-5)**: More focused, faster responses\n",
        "- **Higher values (8-15)**: More comprehensive, detailed responses\n",
        "\n",
        "### Node Postprocessors\n",
        "- **SimilarityPostprocessor**: Filters out low-relevance documents\n",
        "\n",
        "### Similarity Cutoff (0.0-1.0)\n",
        "- **0.1-0.3**: More permissive, includes potentially relevant docs\n",
        "- **0.5-0.8**: More strict, only highly relevant docs\n",
        "\n",
        "### Response Synthesizers\n",
        "- **TreeSummarize**: Hierarchical summarization, good for complex topics\n",
        "- **Refine**: Iterative refinement, builds detailed responses\n",
        "- **CompactAndRefine**: Efficient version of Refine\n",
        "- **Default**: Standard synthesis approach\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚úÖ Assignment Completion Checklist\n",
        "\n",
        "Before submitting, ensure you have:\n",
        "\n",
        "- [ ] Set up your OPENROUTER_API_KEY environment variable\n",
        "- [ ] Imported all necessary libraries including advanced RAG components\n",
        "- [ ] Created AdvancedRAGBackend class with configurable parameters\n",
        "- [ ] Implemented all required methods:\n",
        "  - [ ] `update_settings()` - Updates LLM and chunking parameters\n",
        "  - [ ] `initialize_database()` - Sets up vector database\n",
        "  - [ ] `get_postprocessor()` - Returns selected postprocessor\n",
        "  - [ ] `get_synthesizer()` - Returns selected synthesizer\n",
        "  - [ ] `advanced_query()` - Handles queries with all configuration options\n",
        "- [ ] Created advanced Gradio interface with all required components:\n",
        "  - [ ] Initialize database button\n",
        "  - [ ] Model selection dropdown (gpt-4o, gpt-4o-mini)\n",
        "  - [ ] Temperature slider (0 to 1, step 0.1)\n",
        "  - [ ] Chunk size input (default 512)\n",
        "  - [ ] Chunk overlap input (default 50)\n",
        "  - [ ] Similarity top-k slider (1 to 20, default 5)\n",
        "  - [ ] Node postprocessor multiselect\n",
        "  - [ ] Similarity cutoff slider (0.0 to 1.0, step 0.1, default 0.3)\n",
        "  - [ ] Response synthesizer dropdown\n",
        "  - [ ] Query input and submit button\n",
        "  - [ ] Response output\n",
        "  - [ ] Configuration display\n",
        "- [ ] Connected all components to backend functions\n",
        "- [ ] Successfully launched the application\n",
        "- [ ] Tested different parameter combinations\n",
        "- [ ] Verified all configuration options work correctly\n",
        "\n",
        "## üéä Congratulations!\n",
        "\n",
        "You've successfully built a professional, production-ready RAG application! You now have:\n",
        "\n",
        "- **Advanced Parameter Control**: Full control over all RAG system parameters\n",
        "- **Professional UI**: Clean, organized interface with proper layout\n",
        "- **Real-time Configuration**: Ability to experiment with different settings\n",
        "- **Production Patterns**: Understanding of how to build scalable AI applications\n",
        "\n",
        "## üöÄ Next Steps & Extensions\n",
        "\n",
        "**Potential Enhancements:**\n",
        "1. **Authentication**: Add user login and session management\n",
        "2. **Document Upload**: Allow users to upload their own documents\n",
        "3. **Chat History**: Implement conversation memory\n",
        "4. **Performance Monitoring**: Add response time and quality metrics\n",
        "5. **A/B Testing**: Compare different configurations side-by-side\n",
        "6. **Export Features**: Download responses and configurations\n",
        "7. **Advanced Visualizations**: Show document similarity scores and retrieval paths\n",
        "\n",
        "**Deployment Options:**\n",
        "- **Local**: Run on your machine for development\n",
        "- **Gradio Cloud**: Deploy with `interface.launch(share=True)`\n",
        "- **Hugging Face Spaces**: Deploy to Hugging Face for public access\n",
        "- **Docker**: Containerize for scalable deployment\n",
        "- **Cloud Platforms**: Deploy to AWS, GCP, or Azure\n",
        "\n",
        "You're now ready to build sophisticated AI-powered applications!\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "accelerator",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
