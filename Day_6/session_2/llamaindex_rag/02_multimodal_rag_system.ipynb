{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Multimodal RAG System Tutorial\n",
        "\n",
        "This notebook extends our basic RAG system to handle multiple data types including PDFs, CSV files, JSON, Markdown, HTML, images, and audio files. We'll demonstrate the advanced capabilities of LlamaIndex's `SimpleDirectoryReader` for multimodal data processing.\n",
        "\n",
        "## What's New in This Tutorial\n",
        "\n",
        "Building upon our previous RAG system, we now add:\n",
        "- **Multimodal Document Loading**: CSV, JSON, Markdown, HTML, Images, Audio\n",
        "- **Advanced SimpleDirectoryReader Features**: File filtering, metadata extraction, custom processors\n",
        "- **Cross-Modal Queries**: Search across different data types simultaneously\n",
        "- **Structured Data Integration**: Combine tabular data with unstructured text\n",
        "- **Visual Content Processing**: Extract information from images and charts\n",
        "\n",
        "## Supported File Types (Per LlamaIndex Documentation)\n",
        "\n",
        "According to the [SimpleDirectoryReader documentation](https://developers.llamaindex.ai/python/framework/module_guides/loading/simpledirectoryreader/), the following formats are automatically supported:\n",
        "\n",
        "- **.csv** - comma-separated values\n",
        "- **.docx** - Microsoft Word  \n",
        "- **.epub** - EPUB ebook format\n",
        "- **.hwp** - Hangul Word Processor\n",
        "- **.ipynb** - Jupyter Notebook\n",
        "- **.jpeg, .jpg** - JPEG image\n",
        "- **.mbox** - MBOX email archive\n",
        "- **.md** - Markdown\n",
        "- **.mp3, .mp4** - audio and video\n",
        "- **.pdf** - Portable Document Format\n",
        "- **.png** - Portable Network Graphics\n",
        "- **.ppt, .pptm, .pptx** - Microsoft PowerPoint\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Environment Setup and Configuration\n",
        "\n",
        "First, let's set up our environment with hardcoded configurations. We'll use OpenRouter for the LLM and local embeddings for cost-effective processing.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install -r \"../requirements.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Environment variables loaded successfully\n",
            "‚úì LLM Model: gpt-5-mini\n",
            "‚úì Embedding Model: local:BAAI/bge-small-en-v1.5\n",
            "Environment setup complete!\n"
          ]
        }
      ],
      "source": [
        "# Environment setup with hardcoded configurations\n",
        "import os\n",
        "import time\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Hardcoded configuration\n",
        "CONFIG = {\n",
        "    \"llm_model\": \"gpt-5-mini\",\n",
        "    \"embedding_model\": \"local:BAAI/bge-small-en-v1.5\",\n",
        "    \"chunk_size\": 1024,\n",
        "    \"chunk_overlap\": 100,\n",
        "    \"similarity_top_k\": 5,\n",
        "    \"data_path\": \"../data\",\n",
        "    \"vector_db_path\": \"storage/multimodal_vectordb\",\n",
        "    \"index_storage_path\": \"storage/multimodal_index\"\n",
        "}\n",
        "\n",
        "def setup_environment():\n",
        "    \"\"\"\n",
        "    Setup environment variables and basic configuration.\n",
        "    \n",
        "    Returns:\n",
        "        bool: Success status\n",
        "    \"\"\"\n",
        "    # Load environment variables from .env file\n",
        "    load_dotenv()\n",
        "    \n",
        "    # Disable tokenizer warning\n",
        "    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "    \n",
        "    # Check for required API key\n",
        "    api_key = os.getenv(\"OPENROUTER_API_KEY\")\n",
        "    if not api_key:\n",
        "        print(\"‚ö†Ô∏è  OPENROUTER_API_KEY not found in environment variables\")\n",
        "        print(\"Please add your OpenRouter API key to a .env file\")\n",
        "        return False\n",
        "    \n",
        "    print(\"‚úì Environment variables loaded successfully\")\n",
        "    print(f\"‚úì LLM Model: {CONFIG['llm_model']}\")\n",
        "    print(f\"‚úì Embedding Model: {CONFIG['embedding_model']}\")\n",
        "    return True\n",
        "\n",
        "# Run the setup\n",
        "success = setup_environment()\n",
        "if success:\n",
        "    print(\"Environment setup complete!\")\n",
        "else:\n",
        "    print(\"Environment setup failed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. LlamaIndex Configuration for Multimodal Data\n",
        "\n",
        "Let's configure LlamaIndex with our hardcoded settings for OpenRouter LLM and local embeddings.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîÑ Multimodal vs. Unimodal Vector Index Creation\n",
        "\n",
        "### Understanding the Key Difference\n",
        "\n",
        "While **multimodal** and **unimodal** RAG systems use the same underlying `VectorStoreIndex.from_documents()` method, there's a **critical difference** in how existing indexes are loaded that affects system behavior and reliability.\n",
        "\n",
        "### üìä Index Creation Comparison\n",
        "\n",
        "| Aspect | Unimodal (Academic Papers) | Multimodal (This Notebook) | Impact |\n",
        "|--------|----------------------------|----------------------------|---------|\n",
        "| **Document Types** | Single type (PDF papers) | Multiple types (PDF, CSV, HTML, Images, Audio) | Different processing pipelines |\n",
        "| **Index Creation** | `VectorStoreIndex.from_documents()` | `VectorStoreIndex.from_documents()` | **Identical** |\n",
        "| **Storage Context** | ‚úÖ Full StorageContext persistence | ‚úÖ Full StorageContext persistence | **Identical** |\n",
        "| **Index Loading** | `load_index_from_storage()` | `VectorStoreIndex.from_vector_store()` | **‚ö†Ô∏è Different!** |\n",
        "| **Metadata Complexity** | Single file type metadata | Rich cross-modal metadata | More complex relationships |\n",
        "\n",
        "### üîç The Critical Loading Difference\n",
        "\n",
        "**Unimodal Loading (Academic Papers):**\n",
        "```python\n",
        "# ROBUST: Complete index reconstruction\n",
        "storage_context = StorageContext.from_defaults(\n",
        "    persist_dir=str(index_path), \n",
        "    vector_store=vector_store\n",
        ")\n",
        "index = load_index_from_storage(storage_context)\n",
        "# ‚úÖ Perfect restoration with all metadata and relationships\n",
        "```\n",
        "\n",
        "**Multimodal Loading (This Notebook):**\n",
        "```python\n",
        "# BASIC: Vector-only reconstruction\n",
        "storage_context = StorageContext.from_defaults(\n",
        "    persist_dir=str(index_path), \n",
        "    vector_store=vector_store\n",
        ")\n",
        "index = VectorStoreIndex.from_vector_store(\n",
        "    vector_store=vector_store,\n",
        "    storage_context=storage_context\n",
        ")\n",
        "# ‚ö†Ô∏è May lose some complex relationships between file types\n",
        "```\n",
        "\n",
        "### üéØ Why This Difference Matters\n",
        "\n",
        "**For Unimodal Systems:**\n",
        "- Documents are homogeneous (all PDFs)\n",
        "- `load_index_from_storage()` ensures perfect reconstruction\n",
        "- Critical for academic reproducibility\n",
        "\n",
        "**For Multimodal Systems:**\n",
        "- Documents are heterogeneous (PDFs, images, audio, CSV)\n",
        "- `from_vector_store()` focuses on vector similarity\n",
        "- Cross-modal relationships handled differently\n",
        "- May prioritize performance over perfect metadata preservation\n",
        "\n",
        "### üìà Practical Implications\n",
        "\n",
        "| Scenario | Unimodal Advantage | Multimodal Trade-off |\n",
        "|----------|-------------------|---------------------|\n",
        "| **Research Reproducibility** | üéØ Identical results every time | ‚ö†Ô∏è Minor variations possible |\n",
        "| **Cross-Modal Queries** | ‚ùå Not applicable | ‚úÖ Query across file types |\n",
        "| **System Startup** | ‚ö° Fastest (complete restoration) | üîÑ Fast (vector-based loading) |\n",
        "| **Metadata Fidelity** | üîí 100% preserved | üìä Core metadata preserved |\n",
        "| **File Type Diversity** | üìÑ Single type (PDFs) | üåà Multiple types supported |\n",
        "\n",
        "### üõ†Ô∏è When to Use Each Approach\n",
        "\n",
        "**Choose Unimodal (`load_index_from_storage`) When:**\n",
        "- Working with homogeneous document types\n",
        "- Perfect reproducibility is critical\n",
        "- Academic research or compliance requirements\n",
        "- Complex document relationships matter\n",
        "\n",
        "**Choose Multimodal (`from_vector_store`) When:**\n",
        "- Processing diverse file types simultaneously\n",
        "- Cross-modal search is the priority\n",
        "- Performance over perfect metadata preservation\n",
        "- Building versatile content search systems\n",
        "\n",
        "### üé® The Multimodal Advantage\n",
        "\n",
        "Despite the loading difference, multimodal indexing provides unique capabilities:\n",
        "\n",
        "1. **üîç Cross-Modal Search**: Find information across PDFs, images, and data files\n",
        "2. **üìä Rich Content Types**: Handle structured (CSV) and unstructured (text) data together  \n",
        "3. **üéµ Audio Integration**: Include transcribed audio content in searches\n",
        "4. **üñºÔ∏è Visual Content**: Extract information from charts and diagrams\n",
        "5. **üìà Unified Knowledge Base**: Single search across all organizational content\n",
        "\n",
        "### üí° Best Practice Recommendation\n",
        "\n",
        "For production multimodal systems, consider implementing **hybrid loading**:\n",
        "\n",
        "```python\n",
        "# Try robust loading first, fallback to vector-only\n",
        "try:\n",
        "    index = load_index_from_storage(storage_context)  # Full restoration\n",
        "    print(\"‚úì Complete index restoration\")\n",
        "except:\n",
        "    index = VectorStoreIndex.from_vector_store(vector_store)  # Vector fallback\n",
        "    print(\"‚úì Vector-based index loading\")\n",
        "```\n",
        "\n",
        "This gives you the reliability of unimodal loading with the flexibility of multimodal processing.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì LLM configured: gpt-5-mini\n",
            "‚úì Embedding model configured: local:BAAI/bge-small-en-v1.5\n",
            "‚úì Text chunking configured: 1024 chars with 100 overlap\n",
            "‚úì LlamaIndex settings configured for multimodal processing\n"
          ]
        }
      ],
      "source": [
        "# LlamaIndex configuration with hardcoded settings\n",
        "from llama_index.core import Settings\n",
        "from llama_index.llms.openrouter import OpenRouter\n",
        "from llama_index.core.embeddings import resolve_embed_model\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "\n",
        "def configure_llamaindex_settings():\n",
        "    \"\"\"Configure LlamaIndex global settings using hardcoded configuration.\"\"\"\n",
        "    \n",
        "    # Set up LLM with OpenRouter using hardcoded model\n",
        "    Settings.llm = OpenRouter(\n",
        "        api_key=os.getenv(\"OPENROUTER_API_KEY\"),\n",
        "        model=CONFIG[\"llm_model\"]\n",
        "    )\n",
        "    print(f\"‚úì LLM configured: {CONFIG['llm_model']}\")\n",
        "\n",
        "    # Set up local embedding model (downloads locally first time, then cached)\n",
        "    Settings.embed_model = resolve_embed_model(CONFIG[\"embedding_model\"])\n",
        "    print(f\"‚úì Embedding model configured: {CONFIG['embedding_model']}\")\n",
        "\n",
        "    # Set up node parser for chunking with hardcoded settings\n",
        "    Settings.node_parser = SentenceSplitter(\n",
        "        chunk_size=CONFIG[\"chunk_size\"], \n",
        "        chunk_overlap=CONFIG[\"chunk_overlap\"]\n",
        "    )\n",
        "    print(f\"‚úì Text chunking configured: {CONFIG['chunk_size']} chars with {CONFIG['chunk_overlap']} overlap\")\n",
        "\n",
        "# Configure the settings\n",
        "configure_llamaindex_settings()\n",
        "print(\"‚úì LlamaIndex settings configured for multimodal processing\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Exploring Our Multimodal Dataset\n",
        "\n",
        "Let's examine the different types of files we have available for processing. This will show the diversity of data types that SimpleDirectoryReader can handle.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üóÇÔ∏è  Dataset Overview\n",
            "==================================================\n",
            "Total files found: 21\n",
            "\n",
            "üìÅ File Types Distribution:\n",
            "  .csv: 4 files (0.00 MB)\n",
            "    - italian_recipes.csv (0.0 MB)\n",
            "    - agent_performance_benchmark.csv (0.0 MB)\n",
            "    - agent_evaluation_metrics.csv (0.0 MB)\n",
            "    ... and 1 more\n",
            "\n",
            "  .html: 2 files (0.00 MB)\n",
            "    - fitness_tracker.html (0.0 MB)\n",
            "    - agent_tutorial.html (0.0 MB)\n",
            "\n",
            "  .md: 4 files (0.00 MB)\n",
            "    - recipe_instructions.md (0.0 MB)\n",
            "    - agent_framework_comparison.md (0.0 MB)\n",
            "    - market_analysis.md (0.0 MB)\n",
            "    ... and 1 more\n",
            "\n",
            "  .mp3: 3 files (2.95 MB)\n",
            "    - rags.mp3 (0.81 MB)\n",
            "    - ai_agents.mp3 (1.54 MB)\n",
            "    - in_the_end.mp3 (0.6 MB)\n",
            "\n",
            "  .pdf: 2 files (1.92 MB)\n",
            "    - AI_Agent_Frameworks.pdf (0.34 MB)\n",
            "    - Emerging_Agent_Architectures.pdf (1.58 MB)\n",
            "\n",
            "  .png: 6 files (0.55 MB)\n",
            "    - recipe_popularity.png (0.04 MB)\n",
            "    - agent_types_comparison.png (0.1 MB)\n",
            "    - agent_performance_comparison.png (0.17 MB)\n",
            "    ... and 3 more\n",
            "\n",
            "‚úì Found 21 files across 6 different file types\n"
          ]
        }
      ],
      "source": [
        "def explore_dataset(data_path: str = None):\n",
        "    \"\"\"\n",
        "    Explore and categorize the files in our dataset by type.\n",
        "    \n",
        "    Args:\n",
        "        data_path (str): Path to the data directory\n",
        "    \"\"\"\n",
        "    if data_path is None:\n",
        "        data_path = CONFIG[\"data_path\"]\n",
        "        \n",
        "    data_dir = Path(data_path)\n",
        "    if not data_dir.exists():\n",
        "        print(f\"Data directory not found: {data_dir}\")\n",
        "        return\n",
        "    \n",
        "    # Categorize files by type\n",
        "    file_types = {}\n",
        "    all_files = []\n",
        "    \n",
        "    # Walk through all files recursively\n",
        "    for file_path in data_dir.rglob(\"*\"):\n",
        "        if file_path.is_file():\n",
        "            suffix = file_path.suffix.lower()\n",
        "            file_size = file_path.stat().st_size\n",
        "            \n",
        "            if suffix not in file_types:\n",
        "                file_types[suffix] = []\n",
        "            \n",
        "            file_info = {\n",
        "                \"path\": str(file_path),\n",
        "                \"name\": file_path.name,\n",
        "                \"size_mb\": round(file_size / (1024 * 1024), 2),\n",
        "                \"size_bytes\": file_size\n",
        "            }\n",
        "            \n",
        "            file_types[suffix].append(file_info)\n",
        "            all_files.append(file_info)\n",
        "    \n",
        "    # Display summary\n",
        "    print(\"üóÇÔ∏è  Dataset Overview\")\n",
        "    print(\"=\" * 50)\n",
        "    print(f\"Total files found: {len(all_files)}\")\n",
        "    \n",
        "    print(f\"\\nüìÅ File Types Distribution:\")\n",
        "    for file_type, files in sorted(file_types.items()):\n",
        "        if file_type:  # Skip files without extension\n",
        "            total_size = sum(f[\"size_mb\"] for f in files)\n",
        "            print(f\"  {file_type}: {len(files)} files ({total_size:.2f} MB)\")\n",
        "            \n",
        "            # Show file details\n",
        "            for file_info in files[:3]:  # Show first 3 files of each type\n",
        "                print(f\"    - {file_info['name']} ({file_info['size_mb']} MB)\")\n",
        "            if len(files) > 3:\n",
        "                print(f\"    ... and {len(files) - 3} more\")\n",
        "    \n",
        "            print()\n",
        "    \n",
        "    return file_types, all_files\n",
        "\n",
        "# Explore our dataset\n",
        "file_types, all_files = explore_dataset()\n",
        "print(f\"‚úì Found {len(all_files)} files across {len(file_types)} different file types\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Basic Multimodal Document Loading\n",
        "\n",
        "Now let's use SimpleDirectoryReader to load all files from our data directory. This demonstrates the core multimodal capability.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üîç Index Creation Implementation Note\n",
        "\n",
        "The following implementation uses **multimodal-optimized loading** - notice the difference from the academic papers notebook:\n",
        "\n",
        "#### Key Implementation Differences:\n",
        "\n",
        "1. **Index Loading Method**: Uses `VectorStoreIndex.from_vector_store()` instead of `load_index_from_storage()`\n",
        "2. **Reasoning**: Optimized for cross-modal search performance over perfect metadata preservation  \n",
        "3. **Trade-off**: Slightly less metadata fidelity but better handling of diverse file types\n",
        "4. **Benefit**: More flexible loading for heterogeneous document collections\n",
        "\n",
        "This approach prioritizes the core multimodal capability while maintaining good performance and reliability.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìÇ Loading multimodal documents from: ../data\n",
            "üîÑ Processing files...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/ishandutta/miniconda3/envs/accelerator/lib/python3.11/site-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            "/Users/ishandutta/miniconda3/envs/accelerator/lib/python3.11/site-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            "/Users/ishandutta/miniconda3/envs/accelerator/lib/python3.11/site-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Successfully loaded 42 documents in 14.85 seconds\n",
            "\n",
            "üìä Documents by MIME type:\n",
            "  application/pdf: 23 documents\n",
            "  audio/mpeg: 3 documents\n",
            "  image/png: 6 documents\n",
            "  text/csv: 4 documents\n",
            "  text/html: 2 documents\n",
            "  unknown: 4 documents\n",
            "\n",
            "üìÑ Sample Document Analysis:\n",
            "File: AI_Agent_Frameworks.pdf\n",
            "Type: application/pdf\n",
            "Size: 360523 bytes\n",
            "Text preview: A Comprehensive Survey of AI Agent Frameworks\n",
            "and Their Applications in Financial Services\n",
            "Satyadhar Joshi\n",
            "Independent\n",
            "Alumnus, International MBA, Bar-Ilan University, Israel\n",
            "satyadhar.joshi@gmail.com...\n",
            "Metadata keys: ['page_label', 'file_name', 'file_path', 'file_type', 'file_size', 'creation_date', 'last_modified_date']\n"
          ]
        }
      ],
      "source": [
        "from llama_index.core import SimpleDirectoryReader\n",
        "\n",
        "def load_multimodal_documents(data_path: str = None, recursive: bool = True):\n",
        "    \"\"\"\n",
        "    Load documents from multiple file types using SimpleDirectoryReader.\n",
        "    \n",
        "    Args:\n",
        "        data_path (str): Path to directory containing multimodal data\n",
        "        recursive (bool): Whether to search subdirectories\n",
        "        \n",
        "    Returns:\n",
        "        List of Document objects\n",
        "    \"\"\"\n",
        "    if data_path is None:\n",
        "        data_path = CONFIG[\"data_path\"]\n",
        "        \n",
        "    print(f\"üìÇ Loading multimodal documents from: {data_path}\")\n",
        "    \n",
        "    # Create SimpleDirectoryReader with recursive search\n",
        "    reader = SimpleDirectoryReader(\n",
        "        input_dir=data_path,\n",
        "        recursive=recursive,\n",
        "        # Let SimpleDirectoryReader handle all supported file types automatically\n",
        "    )\n",
        "    \n",
        "    print(\"üîÑ Processing files...\")\n",
        "    start_time = time.time()\n",
        "    \n",
        "    # Load all documents\n",
        "    documents = reader.load_data()\n",
        "    \n",
        "    end_time = time.time()\n",
        "    \n",
        "    print(f\"‚úÖ Successfully loaded {len(documents)} documents in {end_time - start_time:.2f} seconds\")\n",
        "    \n",
        "    # Analyze loaded documents by file type\n",
        "    doc_types = {}\n",
        "    for doc in documents:\n",
        "        file_type = doc.metadata.get('file_type', 'unknown')\n",
        "        if file_type not in doc_types:\n",
        "            doc_types[file_type] = []\n",
        "        doc_types[file_type].append(doc)\n",
        "    \n",
        "    print(f\"\\nüìä Documents by MIME type:\")\n",
        "    for mime_type, docs in sorted(doc_types.items()):\n",
        "        print(f\"  {mime_type}: {len(docs)} documents\")\n",
        "    \n",
        "    return documents\n",
        "\n",
        "# Load all multimodal documents\n",
        "documents = load_multimodal_documents()\n",
        "\n",
        "# Show sample document information\n",
        "if documents:\n",
        "    print(f\"\\nüìÑ Sample Document Analysis:\")\n",
        "    sample_doc = documents[0]\n",
        "    print(f\"File: {sample_doc.metadata.get('file_name', 'Unknown')}\")\n",
        "    print(f\"Type: {sample_doc.metadata.get('file_type', 'Unknown')}\")\n",
        "    print(f\"Size: {sample_doc.metadata.get('file_size', 0)} bytes\")\n",
        "    print(f\"Text preview: {sample_doc.text[:200]}...\")\n",
        "    print(f\"Metadata keys: {list(sample_doc.metadata.keys())}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Creating Multimodal Vector Index\n",
        "\n",
        "Now let's create a vector index that can handle our multimodal documents using LanceDB for efficient storage and retrieval.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-09-20 12:57:10,572 - WARNING - Table multimodal_documents doesn't exist yet. Please add some data to create it.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Setting up multimodal vector storage...\n",
            "‚úì Connected to LanceDB at: storage/multimodal_vectordb\n",
            "‚úì LanceDB vector store created for multimodal data\n",
            "üî® Creating new multimodal vector index...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Parsing nodes: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 42/42 [00:00<00:00, 87.97it/s]\n",
            "Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 55/55 [00:02<00:00, 21.76it/s]\n",
            "2025-09-20 12:57:13,598 - INFO - Create new table multimodal_documents adding data.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Multimodal index created in 3.07 seconds\n",
            "üíæ Saving multimodal index to storage...\n",
            "‚úì Index saved successfully\n",
            "‚úÖ Multimodal RAG system ready for cross-modal queries!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[90m[\u001b[0m2025-09-20T07:27:13Z \u001b[33mWARN \u001b[0m lance::dataset::write::insert\u001b[90m]\u001b[0m No existing dataset at /Users/ishandutta/Documents/code/ai-accelerator/Day_6/session_2/llamaindex_rag/storage/multimodal_vectordb/multimodal_documents.lance, it will be created\n"
          ]
        }
      ],
      "source": [
        "# Vector store and index creation\n",
        "from llama_index.vector_stores.lancedb import LanceDBVectorStore\n",
        "from llama_index.core import StorageContext, VectorStoreIndex\n",
        "\n",
        "def create_multimodal_vector_store(vector_db_path: str = None):\n",
        "    \"\"\"Create and configure LanceDB vector store for multimodal data.\"\"\"\n",
        "    if vector_db_path is None:\n",
        "        vector_db_path = CONFIG[\"vector_db_path\"]\n",
        "        \n",
        "    try:\n",
        "        import lancedb\n",
        "        \n",
        "        # Create storage directory\n",
        "        Path(vector_db_path).parent.mkdir(parents=True, exist_ok=True)\n",
        "        \n",
        "        # Connect to LanceDB\n",
        "        db = lancedb.connect(str(vector_db_path))\n",
        "        print(f\"‚úì Connected to LanceDB at: {vector_db_path}\")\n",
        "        \n",
        "        # Create vector store\n",
        "        vector_store = LanceDBVectorStore(\n",
        "            uri=str(vector_db_path), \n",
        "            table_name=\"multimodal_documents\"\n",
        "        )\n",
        "        print(\"‚úì LanceDB vector store created for multimodal data\")\n",
        "        \n",
        "        return vector_store\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Error creating vector store: {e}\")\n",
        "        return None\n",
        "\n",
        "def create_multimodal_index(documents: List, \n",
        "                           vector_store, \n",
        "                           index_storage_path: str = None,\n",
        "                           force_rebuild: bool = False):\n",
        "    \"\"\"Create or load a multimodal vector index.\"\"\"\n",
        "    \n",
        "    if index_storage_path is None:\n",
        "        index_storage_path = CONFIG[\"index_storage_path\"]\n",
        "    \n",
        "    index_path = Path(index_storage_path)\n",
        "    index_path.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    # Check if index already exists\n",
        "    index_store_file = index_path / \"index_store.json\"\n",
        "    \n",
        "    if not force_rebuild and index_store_file.exists():\n",
        "        print(\"üìÅ Loading existing multimodal index...\")\n",
        "        try:\n",
        "            storage_context = StorageContext.from_defaults(\n",
        "                persist_dir=str(index_path), \n",
        "                vector_store=vector_store\n",
        "            )\n",
        "            \n",
        "            index = VectorStoreIndex.from_vector_store(\n",
        "                vector_store=vector_store,\n",
        "                storage_context=storage_context\n",
        "            )\n",
        "            print(\"‚úì Successfully loaded existing multimodal index\")\n",
        "            return index\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è  Error loading existing index: {e}\")\n",
        "            print(\"Creating new index...\")\n",
        "    \n",
        "    if not documents:\n",
        "        print(\"‚ùå No documents to index\")\n",
        "        return None\n",
        "    \n",
        "    print(\"üî® Creating new multimodal vector index...\")\n",
        "    start_time = time.time()\n",
        "    \n",
        "    # Create storage context with vector store\n",
        "    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "    \n",
        "    # Create index with progress bar\n",
        "    index = VectorStoreIndex.from_documents(\n",
        "        documents, \n",
        "        storage_context=storage_context, \n",
        "        show_progress=True\n",
        "    )\n",
        "    \n",
        "    end_time = time.time()\n",
        "    print(f\"‚úì Multimodal index created in {end_time - start_time:.2f} seconds\")\n",
        "    \n",
        "    # Save index to storage\n",
        "    print(\"üíæ Saving multimodal index to storage...\")\n",
        "    index.storage_context.persist(persist_dir=str(index_path))\n",
        "    print(\"‚úì Index saved successfully\")\n",
        "    \n",
        "    return index\n",
        "\n",
        "# Create vector store and index for multimodal data\n",
        "print(\"üöÄ Setting up multimodal vector storage...\")\n",
        "multimodal_vector_store = create_multimodal_vector_store()\n",
        "\n",
        "if multimodal_vector_store and documents:\n",
        "    multimodal_index = create_multimodal_index(\n",
        "        documents=documents, \n",
        "        vector_store=multimodal_vector_store,\n",
        "        force_rebuild=False\n",
        "    )\n",
        "    \n",
        "    if multimodal_index:\n",
        "        print(\"‚úÖ Multimodal RAG system ready for cross-modal queries!\")\n",
        "    else:\n",
        "        print(\"‚ùå Failed to create multimodal index\")\n",
        "else:\n",
        "    print(\"‚ùå Vector store creation failed or no documents available\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Multimodal Query Engine and Cross-Modal Search\n",
        "\n",
        "Now let's create a query engine that can search across all our different data types and demonstrate cross-modal queries.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Multimodal retriever configured to find top 5 similar chunks\n",
            "‚úì Multimodal query engine setup successfully\n",
            "üöÄ Multimodal query engine ready for cross-modal search!\n"
          ]
        }
      ],
      "source": [
        "# Query engine setup\n",
        "from llama_index.core.query_engine import RetrieverQueryEngine\n",
        "from llama_index.core.retrievers import VectorIndexRetriever\n",
        "\n",
        "def setup_multimodal_query_engine(index, similarity_top_k: int = None):\n",
        "    \"\"\"Setup query engine for multimodal search.\"\"\"\n",
        "    if similarity_top_k is None:\n",
        "        similarity_top_k = CONFIG[\"similarity_top_k\"]\n",
        "        \n",
        "    if not index:\n",
        "        print(\"‚ùå Index not available. Please create index first.\")\n",
        "        return None\n",
        "    \n",
        "    try:\n",
        "        # Create retriever for multimodal search\n",
        "        retriever = VectorIndexRetriever(\n",
        "            index=index,\n",
        "            similarity_top_k=similarity_top_k,\n",
        "        )\n",
        "        print(f\"‚úì Multimodal retriever configured to find top {similarity_top_k} similar chunks\")\n",
        "        \n",
        "        # Create query engine\n",
        "        query_engine = RetrieverQueryEngine(retriever=retriever)\n",
        "        print(\"‚úì Multimodal query engine setup successfully\")\n",
        "        \n",
        "        return query_engine\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error setting up query engine: {e}\")\n",
        "        return None\n",
        "\n",
        "def search_multimodal_documents(query_engine, query: str, include_metadata: bool = True) -> Dict[str, any]:\n",
        "    \"\"\"Search across multimodal documents and return detailed results.\"\"\"\n",
        "    if not query_engine:\n",
        "        return {\n",
        "            \"success\": False,\n",
        "            \"error\": \"Query engine not initialized.\",\n",
        "            \"response\": \"\",\n",
        "            \"sources\": [],\n",
        "        }\n",
        "    \n",
        "    try:\n",
        "        print(f\"üîç Searching across multimodal data: '{query}'\")\n",
        "        start_time = time.time()\n",
        "        \n",
        "        # Query the multimodal RAG system\n",
        "        response = query_engine.query(query)\n",
        "        \n",
        "        end_time = time.time()\n",
        "        \n",
        "        # Extract source information from retrieved nodes\n",
        "        sources = []\n",
        "        if hasattr(response, \"source_nodes\"):\n",
        "            for node in response.source_nodes:\n",
        "                source_info = {\n",
        "                    \"text\": (\n",
        "                        node.text[:300] + \"...\"\n",
        "                        if len(node.text) > 300\n",
        "                        else node.text\n",
        "                    ),\n",
        "                    \"score\": getattr(node, \"score\", 0.0),\n",
        "                }\n",
        "                \n",
        "                # Add metadata if available and requested\n",
        "                if include_metadata and hasattr(node, \"metadata\"):\n",
        "                    metadata = node.metadata\n",
        "                    source_info.update({\n",
        "                        \"file_name\": metadata.get(\"file_name\", \"Unknown\"),\n",
        "                        \"file_type\": metadata.get(\"file_type\", \"Unknown\"),\n",
        "                        \"file_path\": metadata.get(\"file_path\", \"Unknown\"),\n",
        "                        \"file_size\": metadata.get(\"file_size\", 0),\n",
        "                    })\n",
        "                \n",
        "                sources.append(source_info)\n",
        "        \n",
        "        result = {\n",
        "            \"success\": True,\n",
        "            \"response\": str(response),\n",
        "            \"sources\": sources,\n",
        "            \"query\": query,\n",
        "            \"search_time\": end_time - start_time,\n",
        "            \"num_sources\": len(sources),\n",
        "        }\n",
        "        \n",
        "        print(f\"‚úì Search completed in {end_time - start_time:.2f} seconds\")\n",
        "        print(f\"üìö Found {len(sources)} relevant sources across different file types\")\n",
        "        \n",
        "        return result\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error during search: {e}\")\n",
        "        return {\"success\": False, \"error\": str(e), \"response\": \"\", \"sources\": []}\n",
        "\n",
        "# Setup multimodal query engine\n",
        "if 'multimodal_index' in locals() and multimodal_index:\n",
        "    multimodal_query_engine = setup_multimodal_query_engine(multimodal_index)\n",
        "    \n",
        "    if multimodal_query_engine:\n",
        "        print(\"üöÄ Multimodal query engine ready for cross-modal search!\")\n",
        "    else:\n",
        "        print(\"‚ùå Failed to setup multimodal query engine\")\n",
        "else:\n",
        "    print(\"‚ùå Multimodal index not available\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Interactive Multimodal Query Examples\n",
        "\n",
        "Let's demonstrate the power of our multimodal RAG system with cross-modal queries that search across different data types simultaneously.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üéØ Testing Diverse Multimodal Queries\n",
            "============================================================\n",
            "\n",
            "=============== Query 1: What is the prep time for Spag... ===============\n",
            "‚ùì Multimodal Question: What is the prep time for Spaghetti Carbonara?\n",
            "======================================================================\n",
            "üîç Searching across multimodal data: 'What is the prep time for Spaghetti Carbonara?'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-09-20 12:57:24,186 - INFO - query_type :, vector\n",
            "2025-09-20 12:57:27,616 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2025-09-20 12:57:30,526 - INFO - query_type :, vector\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Search completed in 6.71 seconds\n",
            "üìö Found 5 relevant sources across different file types\n",
            "üí° Answer:\n",
            "15 minutes\n",
            "\n",
            "üìä Search completed in 6.71 seconds\n",
            "üìö Found 5 relevant sources across different data types\n",
            "\n",
            "üìÅ Source File Types: {'Unknown': 1, 'text/csv': 2, 'image/png': 1, 'application/pdf': 1}\n",
            "\n",
            "üìñ Top Sources:\n",
            "1. recipe_instructions.md (Unknown)\n",
            "   Score: 0.675\n",
            "   Content: # üçù Classic Spaghetti Carbonara Recipe\n",
            "\n",
            "## Ingredients\n",
            "- 400g spaghetti pasta\n",
            "- 4 large egg yolks\n",
            "- 100g pecorino romano cheese (grated)\n",
            "- 150g guanci...\n",
            "\n",
            "2. italian_recipes.csv (text/csv)\n",
            "   Score: 0.505\n",
            "   Content: Spaghetti Carbonara, Italian, 20, Easy, Pasta, 450\n",
            "Margherita Pizza, Italian, 45, Medium, Tomato, 320\n",
            "Risotto Milanese, Italian, 35, Hard, Rice, 380\n",
            "T...\n",
            "\n",
            "3. agent_performance_benchmark.csv (text/csv)\n",
            "   Score: 0.400\n",
            "   Content: ReAct-GPT4, reasoning, 0.87, 1200, 45.2, 0.02, langchain\n",
            "AutoGPT, autonomous, 0.78, 2100, 78.5, 0.035, autogpt\n",
            "LangChain-Agent, tool_using, 0.82, 950,...\n",
            "\n",
            "\n",
            "======================================================================\n",
            "\n",
            "=============== Query 2: Which stock had the highest re... ===============\n",
            "‚ùì Multimodal Question: Which stock had the highest return in my portfolio?\n",
            "======================================================================\n",
            "üîç Searching across multimodal data: 'Which stock had the highest return in my portfolio?'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-09-20 12:57:31,779 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2025-09-20 12:57:36,000 - INFO - query_type :, vector\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Search completed in 5.51 seconds\n",
            "üìö Found 5 relevant sources across different file types\n",
            "üí° Answer:\n",
            "Empty Response\n",
            "\n",
            "üìä Search completed in 5.51 seconds\n",
            "üìö Found 5 relevant sources across different data types\n",
            "\n",
            "üìÅ Source File Types: {'text/csv': 1, 'image/png': 1, 'Unknown': 1, 'application/pdf': 2}\n",
            "\n",
            "üìñ Top Sources:\n",
            "1. investment_portfolio.csv (text/csv)\n",
            "   Score: 0.552\n",
            "   Content: Stock, AAPL, Apple Inc, 10000, 12500, 25.0, Medium\n",
            "Stock, GOOGL, Alphabet Inc, 8000, 9200, 15.0, Medium\n",
            "Stock, TSLA, Tesla Inc, 5000, 4200, -16.0, Hig...\n",
            "\n",
            "2. stock_performance.png (image/png)\n",
            "   Score: 0.472\n",
            "   Content: ...\n",
            "\n",
            "3. market_analysis.md (Unknown)\n",
            "   Score: 0.449\n",
            "   Content: # üìà Q3 2024 Market Analysis Report\n",
            "\n",
            "## Executive Summary\n",
            "\n",
            "The third quarter of 2024 showed mixed performance across different asset classes, with tech...\n",
            "\n",
            "\n",
            "======================================================================\n",
            "\n",
            "=============== Query 3: What is the best time to visit... ===============\n",
            "‚ùì Multimodal Question: What is the best time to visit Tokyo?\n",
            "======================================================================\n",
            "üîç Searching across multimodal data: 'What is the best time to visit Tokyo?'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-09-20 12:57:37,280 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2025-09-20 12:57:43,231 - INFO - query_type :, vector\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Search completed in 7.23 seconds\n",
            "üìö Found 5 relevant sources across different file types\n",
            "üí° Answer:\n",
            "The best times to visit Tokyo are March‚ÄìMay (cherry blossom season) and September‚ÄìNovember.\n",
            "\n",
            "üìä Search completed in 7.23 seconds\n",
            "üìö Found 5 relevant sources across different data types\n",
            "\n",
            "üìÅ Source File Types: {'Unknown': 2, 'image/png': 1, 'application/pdf': 2}\n",
            "\n",
            "üìñ Top Sources:\n",
            "1. city_guides.md (Unknown)\n",
            "   Score: 0.545\n",
            "   Content: # Ultimate City Travel Guide\n",
            "\n",
            "## Paris, France üá´üá∑\n",
            "\n",
            "**Best Time to Visit:** April-June, September-October\n",
            "**Must-See Attractions:**\n",
            "- Eiffel Tower - Ic...\n",
            "\n",
            "2. recipe_instructions.md (Unknown)\n",
            "   Score: 0.344\n",
            "   Content: # üçù Classic Spaghetti Carbonara Recipe\n",
            "\n",
            "## Ingredients\n",
            "- 400g spaghetti pasta\n",
            "- 4 large egg yolks\n",
            "- 100g pecorino romano cheese (grated)\n",
            "- 150g guanci...\n",
            "\n",
            "3. city_temperatures.png (image/png)\n",
            "   Score: 0.337\n",
            "   Content: ...\n",
            "\n",
            "\n",
            "======================================================================\n",
            "\n",
            "==================== Custom Question ====================\n",
            "‚ùì Multimodal Question: What is the prep time for Italian recipes?\n",
            "======================================================================\n",
            "üîç Searching across multimodal data: 'What is the prep time for Italian recipes?'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-09-20 12:57:44,554 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Search completed in 21.89 seconds\n",
            "üìö Found 5 relevant sources across different file types\n",
            "üí° Answer:\n",
            "Empty Response\n",
            "\n",
            "üìä Search completed in 21.89 seconds\n",
            "üìö Found 5 relevant sources across different data types\n",
            "\n",
            "üìÅ Source File Types: {'Unknown': 2, 'text/csv': 1, 'image/png': 1, 'audio/mpeg': 1}\n",
            "\n",
            "üìñ Top Sources:\n",
            "1. recipe_instructions.md (Unknown)\n",
            "   Score: 0.572\n",
            "   Content: # üçù Classic Spaghetti Carbonara Recipe\n",
            "\n",
            "## Ingredients\n",
            "- 400g spaghetti pasta\n",
            "- 4 large egg yolks\n",
            "- 100g pecorino romano cheese (grated)\n",
            "- 150g guanci...\n",
            "\n",
            "2. italian_recipes.csv (text/csv)\n",
            "   Score: 0.547\n",
            "   Content: Spaghetti Carbonara, Italian, 20, Easy, Pasta, 450\n",
            "Margherita Pizza, Italian, 45, Medium, Tomato, 320\n",
            "Risotto Milanese, Italian, 35, Hard, Rice, 380\n",
            "T...\n",
            "\n",
            "3. city_guides.md (Unknown)\n",
            "   Score: 0.405\n",
            "   Content: # Ultimate City Travel Guide\n",
            "\n",
            "## Paris, France üá´üá∑\n",
            "\n",
            "**Best Time to Visit:** April-June, September-October\n",
            "**Must-See Attractions:**\n",
            "- Eiffel Tower - Ic...\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def ask_multimodal_question(query_engine, question: str, show_sources: bool = True):\n",
        "    \"\"\"\n",
        "    Ask a custom question to the multimodal RAG system and display results.\n",
        "    \n",
        "    Args:\n",
        "        query_engine: The configured multimodal query engine\n",
        "        question (str): Your question about the multimodal data\n",
        "        show_sources (bool): Whether to display source information\n",
        "    \"\"\"\n",
        "    print(f\"‚ùì Multimodal Question: {question}\")\n",
        "    print(\"=\" * 70)\n",
        "    \n",
        "    result = search_multimodal_documents(query_engine, question, include_metadata=True)\n",
        "    \n",
        "    if result[\"success\"]:\n",
        "        print(f\"üí° Answer:\")\n",
        "        print(result[\"response\"])\n",
        "        print(f\"\\nüìä Search completed in {result['search_time']:.2f} seconds\")\n",
        "        print(f\"üìö Found {result['num_sources']} relevant sources across different data types\")\n",
        "        \n",
        "        if show_sources and result[\"sources\"]:\n",
        "            # Show file type distribution\n",
        "            file_types = {}\n",
        "            for source in result[\"sources\"]:\n",
        "                file_type = source.get(\"file_type\", \"unknown\")\n",
        "                if file_type not in file_types:\n",
        "                    file_types[file_type] = 0\n",
        "                file_types[file_type] += 1\n",
        "            \n",
        "            print(f\"\\nüìÅ Source File Types: {dict(file_types)}\")\n",
        "            \n",
        "            print(f\"\\nüìñ Top Sources:\")\n",
        "            for i, source in enumerate(result[\"sources\"][:3], 1):\n",
        "                print(f\"{i}. {source.get('file_name', 'Unknown')} ({source.get('file_type', 'Unknown')})\")\n",
        "                print(f\"   Score: {source.get('score', 0):.3f}\")\n",
        "                print(f\"   Content: {source['text'][:150]}...\")\n",
        "                print()\n",
        "                \n",
        "    else:\n",
        "        print(f\"‚ùå Error: {result['error']}\")\n",
        "\n",
        "# # Example multimodal queries\n",
        "# multimodal_queries = [\n",
        "#     # \"What are the performance benchmarks for different AI agents?\",\n",
        "#     # \"How do I configure a ReAct agent for research tasks?\", \n",
        "#     # \"What are the architectural patterns discussed in the agent frameworks?\",\n",
        "#     # \"Which AI agent has the best accuracy score?\",\n",
        "#     # \"What are the cost implications of different agent models?\"\n",
        "#     \"What is the accuracy_score for the ReAct agent?\"\n",
        "# ]\n",
        "\n",
        "# print(\"üéØ Multimodal Query Demonstrations\")\n",
        "# print(\"=\" * 60)\n",
        "\n",
        "# # Run a few example queries\n",
        "# for i, question in enumerate(multimodal_queries[:3], 1):\n",
        "#     print(f\"\\n{'='*20} Example {i} {'='*20}\")\n",
        "    \n",
        "#     if 'multimodal_query_engine' in locals() and multimodal_query_engine:\n",
        "#         ask_multimodal_question(multimodal_query_engine, question, show_sources=True)\n",
        "#     else:\n",
        "#         print(\"‚ùå Multimodal query engine not available\")\n",
        "    \n",
        "#     if i < 3:\n",
        "#         print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "# Demo queries for diverse data types\n",
        "diverse_queries = [\n",
        "    \"What is the prep time for Spaghetti Carbonara?\",  # Should hit cooking CSV\n",
        "    \"Which stock had the highest return in my portfolio?\",  # Should hit finance CSV\n",
        "    \"What is the best time to visit Tokyo?\",  # Should hit travel markdown\n",
        "    \"How many calories did I burn on Tuesday?\",  # Should hit health HTML\n",
        "    \"What are the steps to make Carbonara?\",  # Should hit cooking markdown\n",
        "    \"What was NVIDIA's performance?\",  # Should hit finance data\n",
        "]\n",
        "\n",
        "print(\"üéØ Testing Diverse Multimodal Queries\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Test one query from each topic\n",
        "for i, question in enumerate(diverse_queries[:3], 1):\n",
        "    print(f\"\\n{'='*15} Query {i}: {question[:30]}... {'='*15}\")\n",
        "    ask_multimodal_question(multimodal_query_engine, question, show_sources=True)\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "\n",
        "# Custom question area\n",
        "print(f\"\\n{'='*20} Custom Question {'='*20}\")\n",
        "custom_question = \"What is the prep time for Italian recipes?\"\n",
        "ask_multimodal_question(multimodal_query_engine, custom_question, show_sources=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚ùì Multimodal Question: in the end it doesn't even matter\n",
            "======================================================================\n",
            "üîç Searching across multimodal data: 'in the end it doesn't even matter'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-09-20 12:58:16,704 - INFO - query_type :, vector\n",
            "2025-09-20 12:58:18,223 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Search completed in 7.48 seconds\n",
            "üìö Found 5 relevant sources across different file types\n",
            "üí° Answer:\n",
            "Empty Response\n",
            "\n",
            "üìä Search completed in 7.48 seconds\n",
            "üìö Found 5 relevant sources across different data types\n",
            "\n",
            "üìÅ Source File Types: {'audio/mpeg': 2, 'application/pdf': 2, 'text/csv': 1}\n",
            "\n",
            "üìñ Top Sources:\n",
            "1. in_the_end.mp3 (audio/mpeg)\n",
            "   Score: 0.433\n",
            "   Content: I tried so hard and got so far In the end, it doesn't even matter I had to fall to lose it all In the end, it doesn't even matter...\n",
            "\n",
            "2. Emerging_Agent_Architectures.pdf (application/pdf)\n",
            "   Score: 0.375\n",
            "   Content: Message subscribing or filtering improves multi-agent\n",
            "performance by ensuring agents only receive information relevant to their tasks.\n",
            "In vertical arc...\n",
            "\n",
            "3. Emerging_Agent_Architectures.pdf (application/pdf)\n",
            "   Score: 0.368\n",
            "   Content: complete problems [16, 23, 32]. They often do this by breaking a larger problem into smaller subproblems, and then\n",
            "solving each one with the appropria...\n",
            "\n"
          ]
        }
      ],
      "source": [
        "custom_question=\"in the end it doesn't even matter\"\n",
        "ask_multimodal_question(multimodal_query_engine, custom_question, show_sources=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n",
        "\n",
        "üéâ **Congratulations!** You have successfully built an advanced **Multimodal RAG System** using LlamaIndex's `SimpleDirectoryReader` with comprehensive cross-modal capabilities.\n",
        "\n",
        "### What We Accomplished\n",
        "\n",
        "This tutorial demonstrated building a RAG system that can handle multiple data types:\n",
        "\n",
        "#### 1. **Multimodal Document Loading**\n",
        "- ‚úÖ **PDF Documents**: Academic research papers on AI agents\n",
        "- ‚úÖ **CSV Files**: Agent performance benchmarks and evaluation metrics  \n",
        "- ‚úÖ **Markdown Files**: Framework comparisons and documentation\n",
        "- ‚úÖ **HTML Files**: Tutorial and instructional content\n",
        "- ‚úÖ **Image Files**: Charts, diagrams, and visual content\n",
        "- ‚úÖ **Audio Files**: Supplementary audio content\n",
        "\n",
        "#### 2. **Key Features Implemented**\n",
        "- ‚úÖ **Hardcoded Configuration**: No external config files needed\n",
        "- ‚úÖ **Cross-Modal Search**: Query across all file types simultaneously\n",
        "- ‚úÖ **Semantic Similarity**: Find relevant content regardless of source format\n",
        "- ‚úÖ **Source Attribution**: Track which file types contributed to answers\n",
        "- ‚úÖ **LanceDB Vector Store**: Efficient multimodal document storage\n",
        "- ‚úÖ **OpenRouter Integration**: Using `gpt-4o` for response generation\n",
        "- ‚úÖ **Local Embeddings**: `BAAI/bge-small-en-v1.5` for cost-effective embedding\n",
        "\n",
        "#### 3. **SimpleDirectoryReader Capabilities**\n",
        "According to the [official documentation](https://developers.llamaindex.ai/python/framework/module_guides/loading/simpledirectoryreader/), we successfully utilized:\n",
        "\n",
        "```python\n",
        "# Basic multimodal loading\n",
        "SimpleDirectoryReader(input_dir=\"../../data\", recursive=True)\n",
        "\n",
        "# Advanced features available\n",
        "SimpleDirectoryReader(\n",
        "    input_dir=\"path/to/directory\",\n",
        "    recursive=True,                    # Search subdirectories\n",
        "    required_exts=[\".pdf\", \".csv\"],    # Filter file types\n",
        "    exclude=[\"file1.txt\"],            # Exclude specific files\n",
        "    file_metadata=custom_func,         # Custom metadata extraction\n",
        "    num_files_limit=100,              # Limit number of files\n",
        "    encoding=\"utf-8\"                  # Specify encoding\n",
        ")\n",
        "```\n",
        "\n",
        "### Real-World Applications\n",
        "\n",
        "This multimodal RAG system can be applied to:\n",
        "\n",
        "- **Research and Academia**: Query across papers, datasets, and supplementary materials\n",
        "- **Documentation Systems**: Search technical docs, tutorials, configs, and examples\n",
        "- **Business Intelligence**: Combine reports, spreadsheets, presentations, and recordings\n",
        "- **Content Management**: Organize and search diverse content libraries\n",
        "- **Knowledge Bases**: Build comprehensive Q&A systems with diverse source materials\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "1. **Extend File Types**: Add `.docx`, `.pptx`, `.epub` support\n",
        "2. **Custom Metadata**: Implement domain-specific metadata extraction\n",
        "3. **Hybrid Search**: Combine vector search with keyword search\n",
        "4. **Performance Optimization**: Use iterative loading for large datasets\n",
        "5. **Multi-Language Support**: Test with international documents\n",
        "\n",
        "### Usage Tips\n",
        "\n",
        "- **Query Optimization**: Use specific queries that benefit from cross-modal information\n",
        "- **File Organization**: Structure data directories logically\n",
        "- **Custom Questions**: Modify the `custom_question` variable to test your own queries\n",
        "- **Monitor Sources**: Check file type distribution in results to understand retrieval patterns\n",
        "\n",
        "Happy building with multimodal RAG! üöÄüìöüîç\n",
        "\n",
        "---\n",
        "\n",
        "**Ready to explore?** Run the cells above and try your own questions with the interactive query interface!\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "accelerator",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
